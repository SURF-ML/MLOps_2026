# ðŸš€ vLLM Inference & Embedding Guide (Qwen3) on Snellius

This guide explains how to perform **Batch Inference** (generating text) and **Batch Embedding** (converting text to vectors) using the Qwen3 model family on the Snellius `gpu_course` partition.

## ðŸ“‚ Project Structure
All commands should be executed from the **`src/`** directory to ensure paths remain consistent, but you can change this how you want:
```text
vllm/src/
â”œâ”€â”€ data/                 # Input (.jsonl) and Output (.jsonl, .pkl)
â”œâ”€â”€ python_scripts/       # Python client scripts (Decoder & Encoder)
â”œâ”€â”€ slurm_jobs/           # SLURM submission scripts (.job)
â”œâ”€â”€ DECODER_README.md     # This guide
â””â”€â”€ ENCODER_README.md     # Encoder specific details
```

> **Note:** Remember to adjust the parameters in the `.job` files (such as `--time`, `--mem`, and `--cpus-per-task`) to suit the size and complexity of your own dataset. And also remember to change paths to your data, right now the script works as an example! 

---

## ðŸ“¥ 1. Data Requirements
All text data **must** be provided in `.jsonl` (JSON Lines) format. Each line must be a standalone JSON object.

* **Format Requirement:** Every piece of text information you wish to process should be structured exactly like the example below.
* **Decoder Input:** Must contain a `"prompt"` key.
* **Encoder Input:** Flexible. It looks for text in this priority: `report` â†’ `response` â†’ `text` â†’ `prompt`.

**Example: `data/prompts.jsonl`**
```json
{"prompt": "What is Barrett's Oesophagus?"}
{"prompt": "List the main risk factors for developing Barrett's Oesophagus."}
{"prompt": "How is Barrett's Oesophagus typically diagnosed?"}
```

---

## âš™ï¸ 2. Running Batch Inference (The Decoder)
This step takes your prompts and generates text responses using the **Qwen3-4B-AWQ** model.

### Controlling "Creativity" (Temperature)
You can modify the `temperature` parameter within the `.job` script or Python call to change the nature of the output:
* **Low Temperature (e.g., 0.1 - 0.6):** Makes the model **deterministic**. Useful for factual, consistent answers. *Beware: very low values can lead to repetitive, looping text.*
* **High Temperature (e.g., 0.8 - 1.8):** Makes the model **stochastic** (creative/random). Useful for brainstorming or varied phrasing.

### Submission
```bash
sbatch slurm_jobs/batch_inference_decoder.job
```

### ðŸ“¤ Decoder Output
The script produces a new `.jsonl` file in `data/`. It preserves your original prompt and adds a `"response"` field:
```json
{"prompt": "What is Barrett's Oesophagus?", "report": "Barrett's oesophagus is a condition where...", "mode": "no_think"}
```

---

## ðŸ§  3. Running Batch Embedding (The Encoder)
This step converts text into high-dimensional vectors using the **Qwen3-Embedding-0.6B** model.

### Task Descriptions & Instructions
The embedding script uses a `--task_description` string. This is **prepended** to every piece of text in your `.jsonl` before it is embedded. This helps the model understand the context of the vector it is creating.

**Example execution logic within the job script:**
```bash
apptainer exec --nv -B "$PWD" "$CONTAINER" \
  python3 ./python_scripts/batch_inference_encoder.py \
  --input "./data/results.jsonl" \
  --output "./data/embeddings.pkl" \
  --is_query \
  --task_description "Given the following, answer as if you were an experienced expert pathologist." \
  --concurrency 32
```

### Submission
To submit the embedding job to the cluster, run:
```bash
sbatch slurm_jobs/batch_inference_encoder.job
```

### ðŸ”— "Piping" Outputs
A key feature of this setup is that you can **pipe the output of the Decoder directly into the Encoder**. Since the Decoder output contains a `"report"` field and the Encoder prioritizes that field, you should aim to chain the two slurm jobs into one.

Make sure to create your own job script for this piping!

---

## ðŸ“¤ 4. Output Formats
| Component | Format | Content | Usage |
| :--- | :--- | :--- | :--- |
| **Decoder** | `.jsonl` | Original prompt + Model response | Human readable text |
| **Encoder** | `.pkl` | (pid -> embedding) vector dictionary | Mathematical analysis/Similarity search |

**Alignment Note:** The pids in the `.pkl` file should match the pids in the `.jsonl` exactly.

---

## ðŸ”— Useful Links
* **Main Model:** [Qwen/Qwen3-4B-AWQ (HuggingFace)](https://huggingface.co/Qwen/Qwen3-4B-AWQ)
* **Embedding Model:** [Qwen/Qwen3-Embedding-0.6B (HuggingFace)](https://huggingface.co/Qwen/Qwen3-Embedding-0.6B)
* **vLLM Deployment:** [Qwen Documentation for vLLM](https://qwen.readthedocs.io/en/latest/deployment/vllm.html)
* **SURF Documentation:** [LLM inference on Snellius with vLLM](https://servicedesk.surf.nl/wiki/spaces/WIKI/pages/232851290/LLM+inference+on+Snellius+with+vLLM)


